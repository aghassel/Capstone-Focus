{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features names\n",
    "1) navigate - run car detection model\n",
    "2) sign language - run ASL code\n",
    "3) translate - translate speech to text\n",
    "4) transcribe - transcribe speech to text\n",
    "5) question - activate groq endpoint\n",
    "6) object - activate GPT-4Vision to describe items or images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pvporcupine\n",
    "import pyaudio\n",
    "import struct\n",
    "import wave\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_keyword_path = 'focus_win.ppn'\n",
    "\n",
    "porcupine = pvporcupine.create(\n",
    "    access_key=os.environ.get('PORCUPINE_API_KEY'),\n",
    "    keyword_paths=[custom_keyword_path]\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed-length audio recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_audio(duration=2, filename=\"output.wav\"):\n",
    "#     \"\"\"\n",
    "#     Record audio from the default microphone for the given duration\n",
    "#     and save it to the specified filename.\n",
    "#     \"\"\"\n",
    "#     pa = pyaudio.PyAudio()\n",
    "\n",
    "#     stream = pa.open(format=pyaudio.paInt16, channels=1, rate=16000,\n",
    "#                      input=True, frames_per_buffer=1024)\n",
    "\n",
    "#     print(f\"Recording for {duration} seconds...\")\n",
    "\n",
    "#     frames = []\n",
    "\n",
    "#     for _ in range(0, int(16000 / 1024 * duration)):\n",
    "#         data = stream.read(1024)\n",
    "#         frames.append(data)\n",
    "\n",
    "#     print(\"Recording finished.\")\n",
    "\n",
    "#     stream.stop_stream()\n",
    "#     stream.close()\n",
    "#     pa.terminate()\n",
    "\n",
    "#     with wave.open(filename, 'wb') as wf:\n",
    "#         wf.setnchannels(1)\n",
    "#         wf.setsampwidth(pa.get_sample_size(pyaudio.paInt16))\n",
    "#         wf.setframerate(16000)\n",
    "#         wf.writeframes(b''.join(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts(text):\n",
    "    response = client.audio.speech.with_streaming_response.create(\n",
    "                model=\"tts-1\",\n",
    "                voice='nova',\n",
    "                input=text,\n",
    "                response_format=\"wav\"\n",
    "            )\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=22050,\n",
    "                    output=True)\n",
    "    with response as res:\n",
    "        if res.status_code == 200:\n",
    "            for chunk in res.iter_bytes(chunk_size=2048):\n",
    "                stream.write(chunk)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records until a pause is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_until_pause(threshold=500, pause_duration=3):\n",
    "    \"\"\"\n",
    "    Continuously record audio from the microphone until a pause is detected.\n",
    "    \n",
    "    :param threshold: The volume threshold below which is considered silence.\n",
    "    :param pause_duration: The duration of silence in seconds to consider as a pause.\n",
    "    \"\"\"\n",
    "    pa = pyaudio.PyAudio()\n",
    "\n",
    "    stream = pa.open(format=pyaudio.paInt16, channels=1, rate=16000,\n",
    "                     input=True, frames_per_buffer=1024)\n",
    "\n",
    "    print(\"Start speaking...\")\n",
    "\n",
    "    frames = []\n",
    "    silent_frames = 0\n",
    "    pause_frames = int(16000 / 1024 * pause_duration)\n",
    "    \n",
    "    while True:\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "\n",
    "        # Check volume\n",
    "        amplitude = np.frombuffer(data, np.int16)\n",
    "        volume = np.sqrt(np.mean(amplitude**2))\n",
    "\n",
    "        if volume < threshold:\n",
    "            silent_frames += 1\n",
    "        else:\n",
    "            silent_frames = 0\n",
    "\n",
    "        if silent_frames >= pause_frames:\n",
    "            print(\"Pause detected, processing audio.\")\n",
    "            break\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "\n",
    "    filename = \"output.wav\"\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(pa.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(16000)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_feature(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            prompt = \"Transcribe the following audio clip in one or two words, present-tense:\"\n",
    "        )\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            prompt = \"Transcribe the following audio clip in one or two words:\"\n",
    "        )\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def transcribe_mode(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            prompt = \"Transcribe the following audio clip:\"\n",
    "        )\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def translate_mode(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        translation = client.audio.translations.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            prompt = \"Translate the following audio clip into english:\"\n",
    "        )\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groq Mode - Deprecated due to poor inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# def question_mode(transcript):\n",
    "#     tic = time.time()\n",
    "#     stream = groq.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": \"you are a helpful assistant. provide brief responses in around 10 words.\"\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": f\"{transcript}\",\n",
    "#             }\n",
    "#         ],\n",
    "\n",
    "#         model=\"llama2-70b-4096\",\n",
    "#         #model = \"mixtral-8x7b-32768\",\n",
    "\n",
    "#         max_tokens=100,\n",
    "#         stream = True,\n",
    "#     )\n",
    "#     toc = time.time()\n",
    "#     print (f\"Time taken for question mode: {toc-tic}\")\n",
    "#     for chunk in stream:\n",
    "#         print(chunk.choices[0].delta.content, end=\"\")\n",
    "#     #return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object - Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def capture_image(save_path='output.jpg', width=640, height=480, quality=90):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        return False\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        time.sleep(1)\n",
    "        cv2.imwrite(save_path, frame, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
    "        break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return True\n",
    "\n",
    "def encode_image(img_path):\n",
    "    with open(img_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def object_mode(image_path = \"output.jpg\"):\n",
    "    base64_image = encode_image(image_path)\n",
    "    api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"In around 10 words, describe the object in the image.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"low\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        item = response_json.get('choices', [])[0].get('message', {}).get('content', '')\n",
    "        return item\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_mode(question):\n",
    "    response = \"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant. provide brief responses in around 10 words.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question,\n",
    "                }\n",
    "            ],\n",
    "        stream=True,\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "            response += chunk.choices[0].delta.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for the wake word...\n",
      "Wake word detected!\n",
      "Start speaking...\n",
      "Pause detected, processing audio.\n",
      "i don't think it charges object mode yeah\n",
      "\n",
      "Object Detection Mode...\n"
     ]
    }
   ],
   "source": [
    "pa = pyaudio.PyAudio()\n",
    "audio_stream = pa.open(rate=porcupine.sample_rate, channels=1,\n",
    "                       format=pyaudio.paInt16, input=True,\n",
    "                       frames_per_buffer=porcupine.frame_length)\n",
    "\n",
    "print(\"Listening for the wake word...\")\n",
    "\n",
    "while True:\n",
    "    pcm = audio_stream.read(porcupine.frame_length)\n",
    "    pcm = struct.unpack_from(\"h\" * porcupine.frame_length, pcm)\n",
    "\n",
    "    if porcupine.process(pcm) >= 0:\n",
    "        print(\"Wake word detected!\")\n",
    "        break\n",
    "\n",
    "audio_stream.close()\n",
    "pa.terminate()\n",
    "\n",
    "record_until_pause()\n",
    "transcription = transcribe_feature(\"output.wav\")\n",
    "if transcription:\n",
    "    #remove punctuation\n",
    "    transcription = transcription.replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\":\", \"\").replace(\";\", \"\").lower()\n",
    "    print(transcription)\n",
    "\n",
    "    if \"navigate\" in transcription:\n",
    "        print(\"Car Detection Mode...\")\n",
    "        #process_car()\n",
    "    elif \"sign\" in transcription:\n",
    "        print(\"Sign Detection Mode...\")\n",
    "        #process_sign()\n",
    "    elif \"translate\" in transcription:\n",
    "        print(\"Translation Mode...\")\n",
    "        #process_translation()\n",
    "    elif \"transcribe\" in transcription:\n",
    "        print(\"Transcription Mode...\")\n",
    "        record_until_pause()\n",
    "        transcribe = transcribe_audio(\"output.wav\")\n",
    "        #process_transcription()\n",
    "    elif \"question\" in transcription:\n",
    "        print(\"Question and Answering...\")\n",
    "        record_until_pause()\n",
    "        question = transcribe_audio(\"output.wav\")\n",
    "        print(question)\n",
    "        answer = question_mode(question)\n",
    "        tts(answer)\n",
    "    elif \"object\" in transcription:\n",
    "        print(\"Object Detection Mode...\")\n",
    "        capture_image()\n",
    "        object = object_mode()\n",
    "        tts(object)\n",
    "    else:\n",
    "        print(\"Unknown command. Please try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A person holding a red apple in a room.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_image()\n",
    "object_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Person wearing large white-framed safety goggles indoors.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_image()\n",
    "object_mode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEC475",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
