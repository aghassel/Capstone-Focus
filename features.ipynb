{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features names\n",
    "1) navigate - run car detection model\n",
    "2) sign language - run ASL code\n",
    "3) translate - translate speech to text\n",
    "4) transcribe - transcribe speech to text\n",
    "5) question - activate groq endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pvporcupine\n",
    "import pyaudio\n",
    "import struct\n",
    "import wave\n",
    "import openai\n",
    "import os\n",
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "custom_keyword_path = 'focus_win.ppn'\n",
    "\n",
    "porcupine = pvporcupine.create(\n",
    "    access_key=os.environ.get('PORCUPINE_API_KEY'),\n",
    "    keyword_paths=[custom_keyword_path]\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed-length audio recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_audio(duration=2, filename=\"output.wav\"):\n",
    "#     \"\"\"\n",
    "#     Record audio from the default microphone for the given duration\n",
    "#     and save it to the specified filename.\n",
    "#     \"\"\"\n",
    "#     pa = pyaudio.PyAudio()\n",
    "\n",
    "#     stream = pa.open(format=pyaudio.paInt16, channels=1, rate=16000,\n",
    "#                      input=True, frames_per_buffer=1024)\n",
    "\n",
    "#     print(f\"Recording for {duration} seconds...\")\n",
    "\n",
    "#     frames = []\n",
    "\n",
    "#     for _ in range(0, int(16000 / 1024 * duration)):\n",
    "#         data = stream.read(1024)\n",
    "#         frames.append(data)\n",
    "\n",
    "#     print(\"Recording finished.\")\n",
    "\n",
    "#     stream.stop_stream()\n",
    "#     stream.close()\n",
    "#     pa.terminate()\n",
    "\n",
    "#     with wave.open(filename, 'wb') as wf:\n",
    "#         wf.setnchannels(1)\n",
    "#         wf.setsampwidth(pa.get_sample_size(pyaudio.paInt16))\n",
    "#         wf.setframerate(16000)\n",
    "#         wf.writeframes(b''.join(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts(text):\n",
    "    response = client.audio.speech.with_streaming_response.create(\n",
    "                model=\"tts-1\",\n",
    "                voice='nova',\n",
    "                input=text,\n",
    "                response_format=\"wav\"\n",
    "            )\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=22050,\n",
    "                    output=True)\n",
    "    with response as res:\n",
    "        if res.status_code == 200:\n",
    "            for chunk in res.iter_bytes(chunk_size=2048):\n",
    "                stream.write(chunk)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records until a pause is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def record_until_pause(threshold=500, pause_duration=3):\n",
    "    \"\"\"\n",
    "    Continuously record audio from the microphone until a pause is detected.\n",
    "    \n",
    "    :param threshold: The volume threshold below which is considered silence.\n",
    "    :param pause_duration: The duration of silence in seconds to consider as a pause.\n",
    "    \"\"\"\n",
    "    pa = pyaudio.PyAudio()\n",
    "\n",
    "    stream = pa.open(format=pyaudio.paInt16, channels=1, rate=16000,\n",
    "                     input=True, frames_per_buffer=1024)\n",
    "\n",
    "    print(\"Start speaking...\")\n",
    "\n",
    "    frames = []\n",
    "    silent_frames = 0\n",
    "    pause_frames = int(16000 / 1024 * pause_duration)\n",
    "    \n",
    "    while True:\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "\n",
    "        # Check volume\n",
    "        amplitude = np.frombuffer(data, np.int16)\n",
    "        volume = np.sqrt(np.mean(amplitude**2))\n",
    "\n",
    "        if volume < threshold:\n",
    "            silent_frames += 1\n",
    "        else:\n",
    "            silent_frames = 0\n",
    "\n",
    "        if silent_frames >= pause_frames:\n",
    "            print(\"Pause detected, processing audio.\")\n",
    "            break\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "\n",
    "    filename = \"output.wav\"\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(pa.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(16000)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_feature(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            prompt = \"Transcribe the following audio clip in one or two words, present-tense:\"\n",
    "        )\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            prompt = \"Transcribe the following audio clip in one or two words:\"\n",
    "        )\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def transcribe_mode(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            prompt = \"Transcribe the following audio clip:\"\n",
    "        )\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def translate_mode(filename):\n",
    "    \"\"\"\n",
    "    Transcribe the specified audio file using OpenAI's Whisper.\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        translation = client.audio.translations.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\",\n",
    "            prompt = \"Translate the following audio clip into english:\"\n",
    "        )\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groq Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_mode(transcript):\n",
    "    chat_completion = groq.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are a helpful assistant. provide brief responses in around 10 words.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{transcript}\",\n",
    "            }\n",
    "        ],\n",
    "\n",
    "        model=\"llama2-70b-4096\",\n",
    "        #model = \"mixtral-8x7b-32768\",\n",
    "\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for the wake word...\n",
      "Wake word detected!\n",
      "Start speaking...\n",
      "Pause detected, processing audio.\n",
      "question\n",
      "\n",
      "Question and Answering...\n",
      "Start speaking...\n",
      "Pause detected, processing audio.\n",
      "How many states are in America?\n",
      "\n",
      "There are 50 states in America.\n"
     ]
    }
   ],
   "source": [
    "pa = pyaudio.PyAudio()\n",
    "audio_stream = pa.open(rate=porcupine.sample_rate, channels=1,\n",
    "                       format=pyaudio.paInt16, input=True,\n",
    "                       frames_per_buffer=porcupine.frame_length)\n",
    "\n",
    "print(\"Listening for the wake word...\")\n",
    "\n",
    "while True:\n",
    "    pcm = audio_stream.read(porcupine.frame_length)\n",
    "    pcm = struct.unpack_from(\"h\" * porcupine.frame_length, pcm)\n",
    "\n",
    "    if porcupine.process(pcm) >= 0:\n",
    "        print(\"Wake word detected!\")\n",
    "        break\n",
    "\n",
    "audio_stream.close()\n",
    "pa.terminate()\n",
    "\n",
    "record_until_pause()\n",
    "transcription = transcribe_feature(\"output.wav\")\n",
    "if transcription:\n",
    "    #remove punctuation\n",
    "    transcription = transcription.replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\":\", \"\").replace(\";\", \"\").lower()\n",
    "    print(transcription)\n",
    "\n",
    "    if \"navigate\" in transcription:\n",
    "        print(\"Car Detection Mode...\")\n",
    "        #process_car()\n",
    "    elif \"sign\" in transcription:\n",
    "        print(\"Sign Detection Mode...\")\n",
    "        #process_sign()\n",
    "    elif \"translate\" in transcription:\n",
    "        print(\"Translation Mode...\")\n",
    "        #process_translation()\n",
    "    elif \"transcribe\" in transcription:\n",
    "        print(\"Transcription Mode...\")\n",
    "        record_until_pause()\n",
    "        transcribe = transcribe_audio(\"output.wav\")\n",
    "        #process_transcription()\n",
    "    elif \"question\" in transcription:\n",
    "        print(\"Question and Answering...\")\n",
    "        record_until_pause()\n",
    "        question = transcribe_audio(\"output.wav\")\n",
    "        print(question)\n",
    "        answer = question_mode(question)\n",
    "        print(answer)\n",
    "        tts(answer)\n",
    "    else:\n",
    "        print(\"Unknown command. Please try again.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEC475",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
